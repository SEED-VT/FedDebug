{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning\n",
    "# !pip install diskcache\n",
    "# !pip install dotmap\n",
    "# !git clone https://github.com/warisgill/FedDebug-Artifact.git\n",
    "# import sys\n",
    "# sys.path.append(\"/content/FedDebug-Artifact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import copy\n",
    "import gc\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from diskcache import Index\n",
    "from dotmap import DotMap\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn.init import (kaiming_normal_, kaiming_uniform_, normal_,\n",
    "                           orthogonal_, trunc_normal_, uniform_,\n",
    "                           xavier_normal_, xavier_uniform_)\n",
    "\n",
    "from faulty_client_localization.FaultyClientLocalization import FaultyClientLocalization\n",
    "from faulty_client_localization.InferenceGuidedInputs import InferenceGuidedInputs\n",
    "from utils.dl_models import ImageClassifer, initialize_model\n",
    "from utils.fl_datasets import *\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='example.log', level=logging.ERROR)\n",
    "logger = logging.getLogger(\"pytorch_lightning\")\n",
    "seed_everything(786)\n",
    "\n",
    "\n",
    "def _trainModel(pl_model, train_dataset, val_dataset, data_config, epochs, checkpoint_path):\n",
    "    trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=epochs, precision=16,\n",
    "                         check_val_every_n_epoch=3, limit_val_batches=0.25, enable_model_summary=None, enable_checkpointing=False, logger=False)\n",
    "    dm = FedDataModule(train_dataset, val_dataset,\n",
    "                       data_config[\"batch_size\"])\n",
    "\n",
    "    trainer.fit(pl_model, dm)\n",
    "    pl_model.cpu()\n",
    "\n",
    "    # this will create the directory if doesnot exist\n",
    "    trainer.save_checkpoint(checkpoint_path)\n",
    "    del trainer\n",
    "    del dm\n",
    "    del train_dataset\n",
    "\n",
    "    dm = None\n",
    "    trainer = None\n",
    "    train_dataset = None\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return pl_model\n",
    "\n",
    "\n",
    "def simulateFL(model_config, data_config, clients2traindatasets, val_dataset, epochs, base_model_ws):\n",
    "    pl_trained_models = {}\n",
    "    for p_key, train_data in clients2traindatasets.items():\n",
    "        print(f\"Training : {p_key}\")\n",
    "        pl_m = ImageClassifer(model_config)\n",
    "        pl_m.model = initialize_model(model_config)\n",
    "        pl_m.model.load_state_dict(copy.deepcopy(base_model_ws))\n",
    "\n",
    "        pl_m = _trainModel(pl_m, train_data, val_dataset, data_config=data_config,\n",
    "                           epochs=epochs, checkpoint_path=p_key)\n",
    "        pl_trained_models[p_key] = pl_m\n",
    "    return pl_trained_models\n",
    "\n",
    "\n",
    "def prepareIIDDataset(dname, dataset_dir, num_clients):\n",
    "    train, valid, num_classes = initializeTrainAndValidationDataset(\n",
    "        dname, data_dir=dataset_dir)\n",
    "    clients_datasets = splitDataSetIntoNClientsIID(train, clients=num_clients)\n",
    "    return clients_datasets, valid, num_classes\n",
    "\n",
    "\n",
    "def prepareNIIDDataset(dname, dataset_dir, num_clients):\n",
    "    train, valid, num_classes = initializeTrainAndValidationDataset(\n",
    "        dname, data_dir=dataset_dir)\n",
    "    clients_datasets = splitDataSetIntoNClientsNonIID(\n",
    "        train, clients=num_clients)\n",
    "    return clients_datasets, valid, num_classes\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    def getFLClientsDatasets():\n",
    "        if args.sampling == \"iid\":\n",
    "            return prepareIIDDataset(args.dataset, dataset_dir, args.clients)\n",
    "        elif args.sampling == \"niid\":\n",
    "            return prepareNIIDDataset(args.dataset, dataset_dir, args.clients)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"Sampling {args.sampling} is not implemented\")\n",
    "\n",
    "    checkpoint_dir = args.storage + args.checkpoints_dir_name\n",
    "    cache_dir = args.storage + args.cache_name\n",
    "\n",
    "    dataset_dir = args.storage + \"datasets/\"\n",
    "\n",
    "    gray_datasets = [\"mnist\", \"fashionmnist\", \"femnist\"]\n",
    "    channels = 3\n",
    "    if args.dataset in gray_datasets:\n",
    "        channels = 1\n",
    "\n",
    "    model_config = {\"model_name\": args.model,\n",
    "                    \"use_pretrained\": args.pretrained, \"lr\": args.lr, \"weight_decay\": args.weight_decay, \"channels\": channels}\n",
    "\n",
    "    data_config = {'name': args.dataset,\n",
    "                   \"batch_size\": args.batch_size}\n",
    "\n",
    "    cache = Index(cache_dir)\n",
    "\n",
    "    faulty_clients_ids = [int(x) for x in args.faulty_clients_ids.split(\",\")]\n",
    "\n",
    "    key2 = f\"{args.sampling}_{model_config['model_name']}_{args.dataset}_clients_{args.clients}_faulty_{faulty_clients_ids}_bsize_{data_config['batch_size']}_epochs_{args.epochs}_lr_{args.lr}\"\n",
    "    key = key2\n",
    "\n",
    "    print(f\"\\n\\n  ***Simulating FL setup {key} ***\")\n",
    "    model_config[\"checkpoint_path\"] = checkpoint_dir + f\"{key}/\"\n",
    "    clientsdatasets, valid, num_classes = getFLClientsDatasets()\n",
    "\n",
    "    faultyclients2datasets = {}\n",
    "    stringID2intID = {}\n",
    "    for faulty_id in faulty_clients_ids:\n",
    "        k = checkpoint_dir + \\\n",
    "            f\"{key}/faulty_client_{faulty_id}_noise_rate_{args.noise_rate}_classes.ckpt\"\n",
    "        faultyclients2datasets[k] = NoisyDataset(copy.deepcopy(\n",
    "            clientsdatasets[faulty_id]), num_classes=num_classes, noise_rate=args.noise_rate)\n",
    "        stringID2intID[k] = faulty_id\n",
    "\n",
    "    normalclients2datasets = {}\n",
    "    for normal_id in range(args.clients):\n",
    "        if normal_id not in faulty_clients_ids:\n",
    "            k = checkpoint_dir + f\"{key}/client_{normal_id}.ckpt\"\n",
    "            normalclients2datasets[k] = clientsdatasets[normal_id]\n",
    "            stringID2intID[k] = normal_id\n",
    "\n",
    "    data_config[\"single_input_shape\"] = valid[0][0].unsqueeze(0).shape\n",
    "    print(f'input shape, {data_config[\"single_input_shape\"]}')\n",
    "    # return\n",
    "\n",
    "    model_config[\"classes\"] = num_classes\n",
    "\n",
    "    base_model = initialize_model(model_config)\n",
    "\n",
    "    temp_d1 = simulateFL(model_config, data_config,\n",
    "                         faultyclients2datasets, valid, epochs=args.epochs, base_model_ws=copy.deepcopy(base_model.state_dict()))\n",
    "\n",
    "    temp_d2 = simulateFL(model_config, data_config,\n",
    "                         normalclients2datasets, valid, epochs=args.epochs, base_model_ws=copy.deepcopy(base_model.state_dict()))\n",
    "\n",
    "    client2models = {**temp_d1,  **temp_d2}\n",
    "\n",
    "    print(f\"Total clients: {len(client2models)}\")\n",
    "\n",
    "    store = {\"all_clients_datasets\": clientsdatasets, \"num_clients\": args.clients,\n",
    "             \"faulty_clients_ids\": faulty_clients_ids, \"epochs\": args.epochs, 'checkpoint_path': model_config['checkpoint_path'], \"model_config\": model_config,\n",
    "             \"data_config\": data_config, 'data_distribution_among_clients': args.sampling, \"args\": args, \"base_model_ws\": copy.deepcopy(base_model.state_dict())}\n",
    "\n",
    "    cache[key] = store\n",
    "\n",
    "    # changing keys to int\n",
    "    client2models = {stringID2intID[k]: v for k, v in client2models.items()}\n",
    "    print(f\"++Training is done: {key}\")\n",
    "    return client2models, store\n",
    "\n",
    "\n",
    "def evaluateFaultLocalization(predicted_faulty_clients_on_each_input, true_faulty_clients):\n",
    "    true_faulty_clients = set(true_faulty_clients)\n",
    "    detection_acc = 0\n",
    "    for pred_faulty_clients in predicted_faulty_clients_on_each_input:\n",
    "        print(f\"+++ Faulty Clients {pred_faulty_clients}\")\n",
    "        correct_localize_faults = len(\n",
    "            true_faulty_clients.intersection(pred_faulty_clients))\n",
    "        acc = (correct_localize_faults/len(true_faulty_clients))*100\n",
    "        detection_acc += acc\n",
    "    fault_localization_acc = detection_acc / \\\n",
    "        len(predicted_faulty_clients_on_each_input)\n",
    "    return fault_localization_acc\n",
    "\n",
    "\n",
    "def runFaultyClientLocalization(client2models, exp2info, num_bugs, random_generator=kaiming_uniform_, apply_transform=True, k_gen_inputs=10, na_threshold=0.003, use_gpu=True):\n",
    "    print(\">  Running FaultyClientLocalization ..\")\n",
    "    input_shape = list(exp2info['data_config']['single_input_shape'])\n",
    "    generate_inputs = InferenceGuidedInputs(client2models, input_shape, randomGenerator=random_generator, apply_transform=apply_transform,\n",
    "                                            dname=exp2info['data_config']['name'], min_nclients_same_pred=5, k_gen_inputs=k_gen_inputs)\n",
    "    selected_inputs, input_gen_time = generate_inputs.getInputs()\n",
    "\n",
    "    start = time.time()\n",
    "    faultyclientlocalization = FaultyClientLocalization(\n",
    "        client2models, selected_inputs, use_gpu=use_gpu)\n",
    "\n",
    "    potential_benign_clients_for_each_input = faultyclientlocalization.runFaultLocalization(\n",
    "        na_threshold, num_bugs=num_bugs)\n",
    "    fault_localization_time = time.time()-start\n",
    "    return potential_benign_clients_for_each_input, input_gen_time, fault_localization_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# args = DotMap()\n",
    "# args.sampling = \"iid\"\n",
    "# args.cache_name = \"cache/fl_multi_faulty_scale/\"\n",
    "# args.checkpoints_dir_name = \"checkpoints_multi_faulty_scale/\"\n",
    "# args.model = \"resnet50\"\n",
    "# args.pretrained = 1\n",
    "# args.epochs = 10\n",
    "# args.lr = 0.001\n",
    "# args.weight_decay = 0.0001\n",
    "# args.dataset = \"cifar10\"\n",
    "# args.batch_size = 512\n",
    "# args.clients = 30\n",
    "# args.faulty_clients_ids = \"0,1,4\"\n",
    "# args.storage = \"../storage/\"\n",
    "# args.noise_rate = 1  # noise rate\n",
    "# c2ms, exp2info = main(args)\n",
    "\n",
    "\n",
    "# #%%\n",
    "# temp = {k: v.model.eval() for k, v in c2ms.items()}\n",
    "# potential_faulty_clients, _, _ = runFaultyClientLocalization(\n",
    "#     client2models=temp, exp2info=exp2info, num_bugs=len(exp2info['faulty_clients_ids']))\n",
    "# acc = evaluateFaultLocalization(\n",
    "#     potential_faulty_clients, exp2info['faulty_clients_ids'])\n",
    "# print(f\"Fault Localization Acc: {acc}\")\n",
    "\n",
    "# # %%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4:  Global model (ResNet-34) prediction accuracy in the presence of a faulty client with different noise rates. Lower noise rates hardly degrade global model performance.\n",
    "\n",
    "Complete: Resnet-34, 9 Benign and one Faulty Client, CIFAR-10, FEMNIST, default: IID  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 1:  FEDDEBUG’s debugging time and accuracy when localizing a faulty client in 36 different FL settings with 100 test inputs.\n",
    "Repretaive settings 30 clients with Resnet-50 on cifar10 and Femnist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DotMap()\n",
    "args.sampling = \"iid\"\n",
    "args.cache_name = \"cache/fl_multi_faulty_scale/\"\n",
    "args.checkpoints_dir_name = \"checkpoints_multi_faulty_scale/\"\n",
    "args.model = \"resnet50\"\n",
    "args.pretrained = 1\n",
    "args.epochs = 10\n",
    "args.lr = 0.001\n",
    "args.weight_decay = 0.0001\n",
    "args.dataset = \"cifar10\"\n",
    "args.batch_size = 512\n",
    "args.clients = 30\n",
    "args.faulty_clients_ids = \"0,1,4\"\n",
    "args.storage = \"../storage/\"\n",
    "args.noise_rate = 1  # noise rate\n",
    "c2ms, exp2info = main(args)\n",
    "\n",
    "\n",
    "#%%\n",
    "temp = {k: v.model.eval() for k, v in c2ms.items()}\n",
    "potential_faulty_clients, _, _ = runFaultyClientLocalization(\n",
    "    client2models=temp, exp2info=exp2info, num_bugs=len(exp2info['faulty_clients_ids']))\n",
    "acc = evaluateFaultLocalization(\n",
    "    potential_faulty_clients, exp2info['faulty_clients_ids'])\n",
    "print(f\"Fault Localization Acc: {acc}\")\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 7: FEDDEBUG localization performance when a faulty client has varying fault strength (i.e., low noise rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 2: FEDDEBUG’s fault localization in 32 FL configurations with multiple faulty clients, ranging from two to seven.\n",
    "\n",
    "Clients 30, Faulty 5, Resnet-50, Cifar10\n",
    "\n",
    "Clients 30, Faulty 7, Resnet-50, FEMNIST\n",
    "\n",
    "Clients 30, Faulty 7, Densenet-121, Cifar10 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig. 8: FEDDEBUG finds multiple faulty clients in a linear time. Total clients are 50 in each graph\n",
    "\n",
    "Customize: Reducing clients to 30\n",
    "\n",
    "Figure b)\n",
    "\n",
    "ResNet-50, CIFAR-10, Number of faulty clients 3, 5, 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 9: FEDDEBUG retains scalability on a large number of clients.\n",
    "FEMNist-densenet 24-400, 2 faulty clients\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 10: FEDDEBUG performance at neuron activation threshold on 30 clients, including five faulty clients.\n",
    "\n",
    "\n",
    "Clients 30, 5 Faulty, \n",
    "\n",
    "a) Resnet-cifar10\n",
    "\n",
    "c) Densenet-cifar10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic: \n",
    "## Flower FL framework,\n",
    "## Dataset: Mnist\n",
    "## Architecture: ResNext or MobileNet\n",
    "## Noise Rate: 0.75\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90b2b3ffc921ecec285732ffefdcd2be049b62625871c2bd1061419cc3e1c031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
