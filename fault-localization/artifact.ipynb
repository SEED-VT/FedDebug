{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Setup  \n",
    "> Make sure you configure notebook with GPU: Click Edit->notebook settings->hardware accelerator->GPU\n",
    "\n",
    "> Uncomment the following cell after opening in Google colab. (Do not uncomment it in local setup.)  \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/SEED-VT/FedDebug/blob/main/fault-localization/artifact.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-lightning\n",
    "# !pip install diskcache\n",
    "# !pip install dotmap\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !git clone https://github.com/SEED-VT/FedDebug.git\n",
    "# # appending the path\n",
    "# import sys\n",
    "# sys.path.append(\"FedDebug/fault-localization/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  ***Simulating FL setup iid_resnet50_femnist_clients_5_faulty_[0]_bsize_512_epochs_2_lr_0.001 ***\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../storage/datasets/FEMNIST/processed/training.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 70\u001b[0m\n\u001b[1;32m     64\u001b[0m args\u001b[39m.\u001b[39msampling \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miid\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m# [iid, \"niid\"] \u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# FL training\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m c2ms, exp2info \u001b[39m=\u001b[39m trainFLMain(args)\n\u001b[1;32m     71\u001b[0m client2models \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval() \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m c2ms\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     73\u001b[0m \u001b[39m# Fault localazation\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/FLSimulation.py:105\u001b[0m, in \u001b[0;36mtrainFLMain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m  ***Simulating FL setup \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m ***\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m model_config[\u001b[39m\"\u001b[39m\u001b[39mcheckpoint_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m checkpoint_dir \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 105\u001b[0m clientsdatasets, valid, num_classes \u001b[39m=\u001b[39m getFLClientsDatasets()\n\u001b[1;32m    107\u001b[0m faultyclients2datasets \u001b[39m=\u001b[39m {}\n\u001b[1;32m    108\u001b[0m stringID2intID \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/FLSimulation.py:73\u001b[0m, in \u001b[0;36mtrainFLMain.<locals>.getFLClientsDatasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetFLClientsDatasets\u001b[39m():\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39msampling \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miid\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 73\u001b[0m         \u001b[39mreturn\u001b[39;00m prepareIIDDataset(args\u001b[39m.\u001b[39;49mdataset, dataset_dir, args\u001b[39m.\u001b[39;49mclients)\n\u001b[1;32m     74\u001b[0m     \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39msampling \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mniid\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     75\u001b[0m         \u001b[39mreturn\u001b[39;00m prepareNIIDDataset(args\u001b[39m.\u001b[39mdataset, dataset_dir, args\u001b[39m.\u001b[39mclients)\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/FLSimulation.py:56\u001b[0m, in \u001b[0;36mprepareIIDDataset\u001b[0;34m(dname, dataset_dir, num_clients)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepareIIDDataset\u001b[39m(dname, dataset_dir, num_clients):\n\u001b[0;32m---> 56\u001b[0m     train, valid, num_classes \u001b[39m=\u001b[39m initializeTrainAndValidationDataset(\n\u001b[1;32m     57\u001b[0m         dname, data_dir\u001b[39m=\u001b[39;49mdataset_dir)\n\u001b[1;32m     58\u001b[0m     clients_datasets \u001b[39m=\u001b[39m splitDataSetIntoNClientsIID(train, clients\u001b[39m=\u001b[39mnum_clients)\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m clients_datasets, valid, num_classes\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/fl_datasets.py:60\u001b[0m, in \u001b[0;36minitializeTrainAndValidationDataset\u001b[0;34m(dataset_name, data_dir)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melif\u001b[39;00m dataset_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfemnist\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     54\u001b[0m     transform \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mCompose([resize_trasnform,\n\u001b[1;32m     55\u001b[0m                                                 torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     56\u001b[0m                                                 torchvision\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39mNormalize(\n\u001b[1;32m     57\u001b[0m                                                     (\u001b[39m0.1307\u001b[39m,), (\u001b[39m0.3081\u001b[39m,))\n\u001b[1;32m     58\u001b[0m                                                 ])\n\u001b[0;32m---> 60\u001b[0m     X_train, y_train, X_test, y_test, net_dataidx_map, traindata_cls_counts \u001b[39m=\u001b[39m partition_data(\n\u001b[1;32m     61\u001b[0m         dataset_name, data_dir, data_dir, \u001b[39m\"\u001b[39;49m\u001b[39mhomo\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m10\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, download\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     62\u001b[0m     num_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(y_train))\n\u001b[1;32m     63\u001b[0m     \u001b[39m# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/niid_utils.py:169\u001b[0m, in \u001b[0;36mpartition_data\u001b[0;34m(dataset, datadir, logdir, partition, n_clients, beta, download, transform)\u001b[0m\n\u001b[1;32m    167\u001b[0m     X_train, y_train, X_test, y_test \u001b[39m=\u001b[39m load_celeba_data(datadir, transform\u001b[39m=\u001b[39mtransform)\n\u001b[1;32m    168\u001b[0m \u001b[39melif\u001b[39;00m dataset \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfemnist\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     X_train, y_train, u_train, X_test, y_test, u_test \u001b[39m=\u001b[39m load_femnist_data(datadir, download, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m    170\u001b[0m \u001b[39m# elif dataset == 'generated':\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m#     X_train, y_train = [], []\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m#     for loc in range(4):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[39m# print(\">> partioning data\")\u001b[39;00m\n\u001b[1;32m    268\u001b[0m n_train \u001b[39m=\u001b[39m y_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/niid_utils.py:126\u001b[0m, in \u001b[0;36mload_femnist_data\u001b[0;34m(datadir, download, transform)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_femnist_data\u001b[39m(datadir, download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    124\u001b[0m     \u001b[39m# transform = transforms.Compose([transforms.ToTensor()])\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     mnist_train_ds \u001b[39m=\u001b[39m FEMNIST(datadir, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, transform\u001b[39m=\u001b[39;49mtransform, target_transform\u001b[39m=\u001b[39;49mtransform, download\u001b[39m=\u001b[39;49mdownload)\n\u001b[1;32m    127\u001b[0m     mnist_test_ds \u001b[39m=\u001b[39m FEMNIST(datadir, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform, target_transform\u001b[39m=\u001b[39mtransform,  download\u001b[39m=\u001b[39mdownload)\n\u001b[1;32m    129\u001b[0m     X_train, y_train, u_train \u001b[39m=\u001b[39m mnist_train_ds\u001b[39m.\u001b[39mdata, mnist_train_ds\u001b[39m.\u001b[39mtargets, mnist_train_ds\u001b[39m.\u001b[39musers_index\n",
      "File \u001b[0;32m~/Github/FedDebug-Artifact/utils/niid_datasets.py:646\u001b[0m, in \u001b[0;36mFEMNIST.__init__\u001b[0;34m(self, root, dataidxs, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     data_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_file\n\u001b[0;32m--> 646\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musers_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_folder, data_file))\n\u001b[1;32m    648\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataidxs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataidxs]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../storage/datasets/FEMNIST/processed/training.pt'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "from dotmap import DotMap\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn.init import kaiming_uniform_ \n",
    "from utils.faulty_client_localization.FaultyClientLocalization import FaultyClientLocalization\n",
    "from utils.faulty_client_localization.InferenceGuidedInputs import InferenceGuidedInputs\n",
    "from utils.FLSimulation import trainFLMain\n",
    "\n",
    "logging.basicConfig(filename='example.log', level=logging.ERROR)\n",
    "logger = logging.getLogger(\"pytorch_lightning\")\n",
    "seed_everything(786)\n",
    "\n",
    "\n",
    "\n",
    "def evaluateFaultLocalization(predicted_faulty_clients_on_each_input, true_faulty_clients):\n",
    "    true_faulty_clients = set(true_faulty_clients)\n",
    "    detection_acc = 0\n",
    "    for pred_faulty_clients in predicted_faulty_clients_on_each_input:\n",
    "        print(f\"+++ Faulty Clients {pred_faulty_clients}\")\n",
    "        correct_localize_faults = len(\n",
    "            true_faulty_clients.intersection(pred_faulty_clients))\n",
    "        acc = (correct_localize_faults/len(true_faulty_clients))*100\n",
    "        detection_acc += acc\n",
    "    fault_localization_acc = detection_acc / \\\n",
    "        len(predicted_faulty_clients_on_each_input)\n",
    "    return fault_localization_acc\n",
    "\n",
    "\n",
    "def runFaultyClientLocalization(client2models, exp2info, num_bugs, random_generator=kaiming_uniform_, apply_transform=True, k_gen_inputs=10, na_threshold=0.003, use_gpu=True):\n",
    "    print(\">  Running FaultyClientLocalization ..\")\n",
    "    input_shape = list(exp2info['data_config']['single_input_shape'])\n",
    "    generate_inputs = InferenceGuidedInputs(client2models, input_shape, randomGenerator=random_generator, apply_transform=apply_transform,\n",
    "                                            dname=exp2info['data_config']['name'], min_nclients_same_pred=5, k_gen_inputs=k_gen_inputs)\n",
    "    selected_inputs, input_gen_time = generate_inputs.getInputs()\n",
    "\n",
    "    start = time.time()\n",
    "    faultyclientlocalization = FaultyClientLocalization(\n",
    "        client2models, selected_inputs, use_gpu=use_gpu)\n",
    "\n",
    "    potential_benign_clients_for_each_input = faultyclientlocalization.runFaultLocalization(\n",
    "        na_threshold, num_bugs=num_bugs)\n",
    "    fault_localization_time = time.time()-start\n",
    "    return potential_benign_clients_for_each_input, input_gen_time, fault_localization_time\n",
    "\n",
    "\n",
    "# ====== Simulation ===== \n",
    "\n",
    "args = DotMap()\n",
    "args.lr = 0.001\n",
    "args.weight_decay = 0.0001\n",
    "args.batch_size = 512\n",
    "\n",
    "args.model = \"resnet50\" # [resnet18, resnet34, resnet50, densenet121, vgg16]\n",
    "args.epochs = 2  # range 10-25\n",
    "args.dataset = \"cifar10\" # ['cifar10', 'femnist']\n",
    "args.clients = 5 # keep under 30 clients and use Resnet18, Resnet34, or Densenet to evaluate on Colab \n",
    "args.faulty_clients_ids = \"0\" # can be multiple clients separated by comma e.g. \"0,1,2\"  but keep under args.clients clients and at max less than 7 \n",
    "args.noise_rate = 1  # noise rate 0 to 1 \n",
    "args.sampling = \"iid\" # [iid, \"niid\"] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FL training\n",
    "c2ms, exp2info = trainFLMain(args)\n",
    "client2models = {k: v.model.eval() for k, v in c2ms.items()}\n",
    "\n",
    "# Fault localazation\n",
    "potential_faulty_clients, _, _ = runFaultyClientLocalization(\n",
    "    client2models=client2models, exp2info=exp2info, num_bugs=len(exp2info['faulty_clients_ids']))\n",
    "acc = evaluateFaultLocalization(\n",
    "    potential_faulty_clients, exp2info['faulty_clients_ids'])\n",
    "print(f\"Fault Localization Acc: {acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "90b2b3ffc921ecec285732ffefdcd2be049b62625871c2bd1061419cc3e1c031"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
